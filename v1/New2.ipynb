{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((800, 500))  # Larger field\n",
    "        pygame.display.set_caption(\"Reinforcement Learning: Car and Ball\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Load images\n",
    "        self.car_image = pygame.image.load(\"car.png\")\n",
    "        self.ball_image = pygame.image.load(\"ball.png\")\n",
    "        self.goal_image = pygame.image.load(\"goal.png\")\n",
    "        self.background_image = pygame.image.load(\"field.jpg\")  # Background\n",
    "\n",
    "        # Scale images\n",
    "        self.car_image = pygame.transform.scale(self.car_image, (80, 100))  # Larger car\n",
    "        self.ball_image = pygame.transform.scale(self.ball_image, (70, 60))  # Larger ball\n",
    "        self.goal_image = pygame.transform.scale(self.goal_image, (150, 100))  # Larger goal\n",
    "        self.background_image = pygame.transform.scale(self.background_image, (800, 500))\n",
    "\n",
    "        # Goal positions\n",
    "        self.goal_left_x, self.goal_left_y = 100, 218  # Left-center goal\n",
    "        self.goal_right_x, self.goal_right_y = 750, 218  # Right-center goal\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.car_x, self.car_y, self.car_angle = self.randomize_car()\n",
    "        self.ball_x, self.ball_y = self.randomize_ball()\n",
    "        self.car_velocity = 0\n",
    "        self.car_angular_velocity = 0\n",
    "        self.ball_velocity_x = 0\n",
    "        self.ball_velocity_y = 0\n",
    "        self.kicked = False  # Track whether the ball has been kicked\n",
    "\n",
    "        self.prev_car_ball_dist = self.get_distance(self.car_x, self.car_y, self.ball_x, self.ball_y)\n",
    "        return self.get_state()\n",
    "\n",
    "    def randomize_car(self):\n",
    "        x = np.random.randint(300, 500)  # Closer to the ball\n",
    "        y = np.random.randint(150, 350)\n",
    "        angle = np.random.uniform(0, 360)\n",
    "        return x, y, angle\n",
    "\n",
    "    def randomize_ball(self):\n",
    "        x = np.random.randint(250, 550)  # Closer to the car\n",
    "        y = np.random.randint(150, 350)\n",
    "        return x, y\n",
    "\n",
    "    def get_distance(self, x1, y1, x2, y2):\n",
    "        return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "\n",
    "    def get_state(self):\n",
    "        car_ball_dist = self.get_distance(self.car_x, self.car_y, self.ball_x, self.ball_y)\n",
    "        return np.array([\n",
    "            self.car_x, self.car_y, self.car_velocity, self.car_angular_velocity, self.car_angle,\n",
    "            self.ball_x, self.ball_y, car_ball_dist, 1 if self.ball_x < self.car_x else -1\n",
    "        ])\n",
    "\n",
    "    def handle_collisions(self):\n",
    "        # Ball-wall collision\n",
    "        if self.ball_x - 35 < 0 or self.ball_x + 35 > 800:\n",
    "            self.ball_velocity_x = -self.ball_velocity_x\n",
    "        if self.ball_y - 35 < 0 or self.ball_y + 35 > 500:\n",
    "            self.ball_velocity_y = -self.ball_velocity_y\n",
    "\n",
    "    def move_car(self, action):\n",
    "        if self.kicked:  # Stop the car after a kick\n",
    "            self.car_velocity = 0\n",
    "            self.car_angular_velocity = 0\n",
    "            return 0\n",
    "\n",
    "        car_speed = 5\n",
    "        car_angular_speed = 5\n",
    "\n",
    "        if action == 0:  # Forward\n",
    "            self.car_velocity = car_speed\n",
    "        elif action == 1:  # Backward\n",
    "            self.car_velocity = -car_speed\n",
    "        elif action == 2:  # Rotate Left\n",
    "            self.car_angular_velocity = -car_angular_speed\n",
    "        elif action == 3:  # Rotate Right\n",
    "            self.car_angular_velocity = car_angular_speed\n",
    "        elif action == 4:  # Forward + Left\n",
    "            self.car_velocity = car_speed\n",
    "            self.car_angular_velocity = -car_angular_speed\n",
    "        elif action == 5:  # Forward + Right\n",
    "            self.car_velocity = car_speed\n",
    "            self.car_angular_velocity = car_angular_speed\n",
    "        elif action == 6:  # Backward + Left\n",
    "            self.car_velocity = -car_speed\n",
    "            self.car_angular_velocity = -car_angular_speed\n",
    "        elif action == 7:  # Backward + Right\n",
    "            self.car_velocity = -car_speed\n",
    "            self.car_angular_velocity = car_angular_speed\n",
    "        else:  # No action\n",
    "            self.car_velocity = 0\n",
    "            self.car_angular_velocity = 0\n",
    "\n",
    "        self.car_angle += self.car_angular_velocity\n",
    "        self.car_x += math.cos(math.radians(self.car_angle)) * self.car_velocity\n",
    "        self.car_y += math.sin(math.radians(self.car_angle)) * self.car_velocity\n",
    "\n",
    "    def move_ball(self):\n",
    "        self.ball_x += self.ball_velocity_x\n",
    "        self.ball_y += self.ball_velocity_y\n",
    "        self.ball_velocity_x *= 0.98  # Friction\n",
    "        self.ball_velocity_y *= 0.98  # Friction\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # If the ball hasn't been kicked, calculate distance to the car\n",
    "        if not self.kicked:\n",
    "            car_ball_dist = self.get_distance(self.car_x, self.car_y, self.ball_x, self.ball_y)\n",
    "            return -5 if car_ball_dist > 50 else 0  # Negative reward if far from the ball\n",
    "\n",
    "        # After kick, calculate distance to goals\n",
    "        dist_left = self.get_distance(self.ball_x, self.ball_y, self.goal_left_x, self.goal_left_y)\n",
    "        dist_right = self.get_distance(self.ball_x, self.ball_y, self.goal_right_x, self.goal_right_y)\n",
    "\n",
    "        if dist_left <= 75 or dist_right <= 75:  # Ball enters a goal\n",
    "            return 500\n",
    "        elif min(dist_left, dist_right) <= 150:  # Ball near a goal\n",
    "            return 100\n",
    "        elif min(dist_left, dist_right) <= 300:  # Ball far from a goal\n",
    "            return 50\n",
    "        else:  # Ball kicked but very far\n",
    "            return -10\n",
    "\n",
    "    def check_ball_stopped(self):\n",
    "        return abs(self.ball_velocity_x) < 0.1 and abs(self.ball_velocity_y) < 0.1\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move car and ball\n",
    "        self.move_car(action)\n",
    "        self.move_ball()\n",
    "\n",
    "        # Handle collisions\n",
    "        self.handle_collisions()\n",
    "\n",
    "        # Check if ball is kicked\n",
    "        car_ball_dist = self.get_distance(self.car_x, self.car_y, self.ball_x, self.ball_y)\n",
    "        if car_ball_dist <= 35 + 50 and not self.kicked:  # Ball radius + car half-width\n",
    "            self.kicked = True\n",
    "            angle = math.atan2(self.ball_y - self.car_y, self.ball_x - self.car_x)\n",
    "            self.ball_velocity_x = math.cos(angle) * 7\n",
    "            self.ball_velocity_y = math.sin(angle) * 7\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward()\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.kicked and self.check_ball_stopped()\n",
    "\n",
    "        # Get the next state\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        # Draw background\n",
    "        self.screen.blit(self.background_image, (0, 0))\n",
    "\n",
    "        # Draw goals\n",
    "        left_goal_rotated = pygame.transform.rotate(self.goal_image, 90)  # Left goal rotated\n",
    "        right_goal_rotated = pygame.transform.rotate(self.goal_image, -90)  # Right goal rotated\n",
    "        self.screen.blit(left_goal_rotated, (self.goal_left_x - 75, self.goal_left_y - 50))\n",
    "        self.screen.blit(right_goal_rotated, (self.goal_right_x - 75, self.goal_right_y - 50))\n",
    "\n",
    "        # Draw ball\n",
    "        self.screen.blit(self.ball_image, (self.ball_x - 35, self.ball_y - 30))\n",
    "\n",
    "        # Draw car\n",
    "        rotated_car = pygame.transform.rotate(self.car_image, -self.car_angle)\n",
    "        car_rect = rotated_car.get_rect(center=(self.car_x, self.car_y))\n",
    "        self.screen.blit(rotated_car, car_rect.topleft)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes=500, render_interval=10):\n",
    "    env = Environment()\n",
    "    state_size = 9  # Updated to match the size of the state array\n",
    "    action_size = 8  # Number of actions\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    batch_size = 32\n",
    "\n",
    "    # Tracking metrics\n",
    "    rewards = []\n",
    "    kicks = []\n",
    "    goals = []\n",
    "    avg_distances = []\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])  # Updated to match the new state size\n",
    "        total_reward = 0\n",
    "        kicked = False\n",
    "        goal = False\n",
    "        episode_kicks = 0\n",
    "        total_distance = 0\n",
    "        step_count = 0\n",
    "\n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])  # Updated to match the new state size\n",
    "\n",
    "            # Check if the ball was kicked\n",
    "            car_ball_dist = env.get_distance(env.car_x, env.car_y, env.ball_x, env.ball_y)\n",
    "            total_distance += car_ball_dist\n",
    "            step_count += 1\n",
    "\n",
    "            if car_ball_dist <= 35 + 50 and not kicked:  # Ball radius + car half-width\n",
    "                kicked = True\n",
    "                episode_kicks += 1\n",
    "\n",
    "            # Check if the goal was achieved\n",
    "            if env.check_ball_stopped() and env.kicked:\n",
    "                goal = True\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if e % render_interval == 0:\n",
    "                env.render()\n",
    "\n",
    "        # Replay memory\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        # Append metrics\n",
    "        rewards.append(total_reward)\n",
    "        kicks.append(episode_kicks)\n",
    "        goals.append(1 if goal else 0)\n",
    "        avg_distances.append(total_distance / step_count if step_count > 0 else 0)\n",
    "\n",
    "        # Generate training log message\n",
    "        log_message = f\"Episode: {e + 1}/{episodes}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.4f}\"\n",
    "        if kicked:\n",
    "            log_message += \", kicked\"\n",
    "        if goal:\n",
    "            log_message += \", goal\"\n",
    "\n",
    "        print(log_message)\n",
    "\n",
    "        # Save the model every 50 episodes\n",
    "        if (e + 1) % 50 == 0:\n",
    "            agent.model.save(f\"dqn_model_episode_{e+1}.h5\")\n",
    "            print(f\"Model saved at episode {e+1}\")\n",
    "\n",
    "    # Plot training metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(rewards, label=\"Rewards\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Rewards per Episode')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(kicks, label=\"Kicks\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Kicks')\n",
    "    plt.title('Kicks per Episode')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(goals, label=\"Goals\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Goals')\n",
    "    plt.title('Goals per Episode')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(avg_distances, label=\"Average Distance\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title('Average Car-to-Ball Distance')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_dqn(episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Trained Model\n",
    "model = tf.keras.models.load_model(\"dqn_model_episode_100.h5\")\n",
    "env = Environment()\n",
    "state_size = 9\n",
    "action_size = 8\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
